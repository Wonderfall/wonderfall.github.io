[{"content":"Passwordless authentication with OpenSSH keys has been the de facto security standard for years. SSH keys are more robust since they\u0026rsquo;re cryptographically sane by default, and are therefore resilient to most bruteforce atacks. They\u0026rsquo;re also easier to manage while enabling a form of decentralized authentication (it\u0026rsquo;s easy and painless to revoke them). So, what\u0026rsquo;s the next step? And more exactly, why would one need something even better?\nWhy? The main problem with SSH keys is that they\u0026rsquo;re not magic: they consist of a key pair, of which the private key is stored on your disk. You should be wary of various exfiltration attempts, depending on your theat model:\nIf your disk is not encrypted, any physical access could compromise your keys. If your private key isn\u0026rsquo;t encrypted, malicious applications could compromise it. Even with both encrypted, malicious applications could register your keystrokes. All these attempts are particularly a thing on desktop platforms, because they don\u0026rsquo;t have a proper sandboxing model. On Windows, non-UWP apps could likely have full access to your .ssh directory. On desktop Linux distributions, sandboxing is also lacking, and the situation is even worse if you\u0026rsquo;re using X.org since it allows apps to spy on each other (and on your keyboard) by design. A first good step would be to only use SSH from a trusted \u0026amp; decently secure system.\nAnother layer of defense would obviously be multi-factor authentification, or the fact that you\u0026rsquo;re relying on a shared secret instead. We can use FIDO2 security keys for that. That way, even if your private key is compromised, the attacker needs physical access to your security key. TOTP is another common 2FA technique, but it\u0026rsquo;s vulnerable to various attacks, and relies on the quality of the implementation on the server.\nHow? Fortunately for us, OpenSSH 8.2 (released in February 2020) introduced native support for FIDO2/U2F. Most OpenSSH distributions should have the middleware set to use the libfido2 library, including portable versions such as the one for Win32.\nBasically, ssh-keygen -t ${key_type}-sk will generate for us a token-backed key pair. The key types that are supported depend on your security key. Newer models should support both ECDSA-P256 (ecdsa-sk) and Ed25519 (ed25519-sk). If the latter is available, you should prefer it.\nClient configuration To get started:\nssh-keygen -t ed25519-sk This will generate a id_ed25519_sk private key and a id_ed25519_sk.pub public key in .ssh. These are defaults, but you can change them if you want. We will call this key pair a \u0026ldquo;handle\u0026rdquo;, because they\u0026rsquo;re not sufficient by themselves to derive the real secret (as you guessed it, the FIDO2 token is needed). ssh-keygen should ask you to touch the key, and enter the PIN prior to that if you did set one (you probably should).\nYou can also generate a resident key (referred to as discoverable credential in the WebAuthn specification):\nssh-keygen -t ed25519-sk -O resident -O application=ssh:user1 As you can see, a few options must be specified:\n-O resident will tell ssh-keygen to generate a resident key, meaning that the private \u0026ldquo;handle\u0026rdquo; key will also be stored on the security key itself. This has security implications, but you may want that to move seamlessly between different computers. In that case, you should absolutely protect your key with a PIN beforehand. -O application=ssh: is necessary to instruct that the resident key will use a particular slot, because the security key will have to index the resident keys (by default, they use ssh: with an empty user ID). If this is not specificed, the next key generation might overwrite the previous one. -O verify-required is optional but instructs that a PIN is required to generate/access the key. Resident keys can be retrieved using ssh-keygen -K or ssh-add -K if you don\u0026rsquo;t want to write them to the disk.\nServer configuration Next, transfer your public key over to the server (granted you have already access to it with a regular key pair):\nssh-copy-id -i ~/.ssh/id_ed25519_sk.pub user@server.domain.tld Ta-da! But one last thing: we need to make sure the server supports this public key format in sshd_config:\nPubkeyAcceptedKeyTypes ssh-ed25519,sk-ssh-ed25519@openssh.com Adding sk-ssh-ed25519@openssh.com to PubkeyAcceptedKeyTypes should suffice. It\u0026rsquo;s best practice to only use the cryptographic primitives that you need, and hopefully ones that are also modern. This isn\u0026rsquo;t a full-on SSH hardening guide, but you should take a look at the configuration file GrapheneOS uses for their servers to give you an idea on a few good practices.\nRestart the sshd service and try to connect to your server using your key handle (by passing -i ~/.ssh/id_ed25519_sk to ssh for instance). If that works for you (your FIDO2 security key should be needed to derive the real secret), feel free to remove your previous keys from .ssh/authorized_keys on your server.\nThat\u0026rsquo;s cool, right? If you don\u0026rsquo;t have a security key, you can buy one from YubiKey (I\u0026rsquo;m very happy with my 5C NFC by the way), Nitrokey, SoloKeys or OnlyKey (to name a few). If you have an Android device with a hardware security module (HSM), such as the Google Pixels equipped with Titan M (Pixel 3+), you could even use them as bluetooth security keys.\nNo reason to miss out on the party if you can afford it!\n","permalink":"https://wonderfall.dev/openssh-fido2/","summary":"Passwordless authentication with OpenSSH keys has been the de facto security standard for years. SSH keys are more robust since they\u0026rsquo;re cryptographically sane by default, and are therefore resilient to most bruteforce atacks. They\u0026rsquo;re also easier to manage while enabling a form of decentralized authentication (it\u0026rsquo;s easy and painless to revoke them). So, what\u0026rsquo;s the next step? And more exactly, why would one need something even better?\nWhy? The main problem with SSH keys is that they\u0026rsquo;re not magic: they consist of a key pair, of which the private key is stored on your disk.","title":"Securing OpenSSH keys with hardware-based authentication (FIDO2)"},{"content":"Containers aren\u0026rsquo;t that new fancy thing anymore, but they were a big deal. And they still are. They are a concrete solution to the following problem:\n- Hey, your software doesn\u0026rsquo;t work\u0026hellip;\n- Sorry, it works on my computer! Can\u0026rsquo;t help you.\nWhether we like them or not, containers are here to stay. Their expressiveness and semantics allow for an abstraction of the OS dependencies that a software has, the latter being often dynamically linked against certain libraries. The developer can therefore provide a known-good environment where it is expected that their software \u0026ldquo;just works\u0026rdquo;. That is particularly useful for development to eliminate environment-related issues, and that is often used in production as well.\nContainers are often perceived as a great tool for isolation, that is, they can provide an isolated workspace that won\u0026rsquo;t pollute your host OS - all that without the overhead of virtual machines. Security-wise: containers, as we know them on Linux, are glorified namespaces at their core. Containers usually share the same kernel with the host, and namespaces is the kernel feature for separating kernel resources across containers (IDs, networks, filesystems, IPC, etc.). Containers also leverage the features of cgroups to separate system resources (CPU, memory, etc.), and security features such as seccomp to restrict syscalls, or MACs (AppArmor, SELinux).\nAt first, it seems that containers may not provide the same isolation boundary as virtual machines. That\u0026rsquo;s fine, they were not designed to. But they can\u0026rsquo;t be simplified to a simple chroot either. We\u0026rsquo;ll see that a \u0026ldquo;container\u0026rdquo; can mean a lot of things, and their definition may vary a lot depending on the implementation: as such, containers are mostly defined by their semantics.\nDocker is dead, long live Docker\u0026hellip; and OCI! When people think of containers, a large group of them may think of Docker. While Docker played a big role in the popularity of containers a few years ago, it didn\u0026rsquo;t introduce the technology: on Linux, LXC did (Linux Containers). In fact, Docker in its early days was a high-level wrapper for LXC which already combined the power of namespaces and cgroups. Docker then replaced LXC with libcontainer which does more or less the same, plus extra features.\nThen, what happened? Open Container Initiative (OCI). That is the current standard that defines the container ecosystem. That means that whether you\u0026rsquo;re using Docker, Podman, or Kubernetes, you\u0026rsquo;re in fact running OCI-compliant tools. That is a good thing, as it saves a lot of interoperability headaches.\nDocker is no longer the monolithic platform it once was. libcontainer was absorbed by runc, the reference OCI runtime. The high-level components of Docker split into different parts related to the upstream Moby project (Docker is the \u0026ldquo;assembled product\u0026rdquo; of the \u0026ldquo;Moby components\u0026rdquo;). When we refer to Docker, we refer in fact at this powerful high-level API that manages OCI containers. By design, Docker is a daemon that communicates with containerd, a lower-level layer, which in turn communicates with the OCI runtime. That also means that you could very well skip Docker altogether and use containerd or even runc directly.\nDocker client \u0026lt;=\u0026gt; Docker daemon \u0026lt;=\u0026gt; containerd \u0026lt;=\u0026gt; containerd-shim \u0026lt;=\u0026gt; runc Podman is an alternative to Docker developed by RedHat, that also intends to be a drop-in replacement for Docker. It doesn\u0026rsquo;t work with a daemon, and can work rootless by design (Docker has support for rootless too, but that is not without caveats). I would largely recommend Podman over Docker for someone who wants a simple tool to run containers and test code on their machine.\nKubernetes (also known as K8S) is the container platform made by Google. It is designed with scaling in mind, and is about running containers across a cluster whereas Docker focuses on packaging containers on a single node. Docker Swarm is the direct alternative to that, but it has never really took off due to the popularity of K8S.\nFor the rest of this article, we will use Docker as the reference for our examples, along with the Compose specification format. Most of these examples can be adapted to other platforms without issues.\nThe nightmare of dependencies Containers are made from images, and images are typically built from a Dockerfile. Images can be built and distributed through OCI registries: Docker Hub, Google Container Registry, GitHub Container Registry, and so on. You can also set up your own private registry as well, but the reality is that people often pull images from these public registries.\nImages, immutability and versioning Images are what make containers, well, containers. Containers made from the same image should behave similary on different machines. Images can have tags, which are useful for software versioning. The usage of generic tags such as latest is often discouraged because it defeats the purpose of the expected behavior of the container. Tags are not necessarily immutable by design, and they shouldn\u0026rsquo;t be (more on that below). Digest, however, is the attribute of an immutable image, and is often generated with the SHA-256 algorithm.\ndocker.io/library/golang:1.17.1@sha256:232a180dbcbcfa7250917507f3827d88a9ae89bb1cdd8fe3ac4db7b764ebb25 ^ ^ ^ ^ | | | | Registry Image Tag Digest (immutable) Now onto why tags shouldn\u0026rsquo;t be immutable: as written above, containers bring us an abstraction over the OS dependencies that are used by the packaged software. That is nice indeed, but this shouldn\u0026rsquo;t lure us into believing that we can forget security updates. The fact is, there is still a whole OS to care about, and we can\u0026rsquo;t just think of the container as a simple package tool for software.\nFor these reasons, good practices were established:\nAn image should be as minimal as possible (Alpine Linux, or scratch/distroless). An image, with a given tag, should be regularly built, without cache to ensure all layers are freshly built. An image should be rebuilt when the images it\u0026rsquo;s based on are updated. A minimal base system Alpine Linux is often the choice for official images for the first reason. This is not a typical Linux distribution as it uses musl as its C library, but it works quite well. Actually, I\u0026rsquo;m quite fond of Alpine Linux and apk (its package manager). If a supervision suite is needed, I\u0026rsquo;d look into s6. If you need a glibc distribution, Debian provides slim variants for lightweight base images. We can do even better than using Alpine by using distroless images, allowing us to have state-of-the-art application containers.\n\u0026ldquo;Distroless\u0026rdquo; is a fancy name referring to an image with a minimal set of dependencies, from none (for fully static binaries) to some common libraries (typically the C library). Google maintains distroless images you can use as a base for your own images. If you were wondering, the difference with scratch (empty starting point) is that distroless images contain common dependencies that \u0026ldquo;almost-statically compiled\u0026rdquo; binaries may need, such as ca-certificates.\nHowever, distroless images are not suited for every application. In my experience though, distroless is an excellent option with pure Go binaries. Going with minimal images drastically reduces the available attack surface in the container. For example, here\u0026rsquo;s a multi-stage Dockerfile resulting in a minimal non-root image for a simple Go project:\nFROM golang:alpine as build WORKDIR /app COPY . . RUN CGO_ENABLED=0 go mod -o /my_app cmd/my_app FROM gcr.io/distroless/static COPY --from=build /my_app / USER nobody ENTRYPOINT [\u0026#34;/my_app\u0026#34;] The main drawback of using minimal images is the lack of tools that help with debugging, which also constitute the very attack surface we\u0026rsquo;re trying to get rid of. The trade-off is probably not worth the hassle for development-focused containers, and if you\u0026rsquo;re running such images in production, you have to be confident enough to operate with them. Note that the gcr.io/distroless images have a :debug tag to help in that regard.\nKeeping images up-to-date The two other points are highly problematic, because most software vendors just publish an image on release, and forget about it. You should take it up to them if you\u0026rsquo;re running images that are versioned but not regularly updated. I\u0026rsquo;d say running scheduled builds once a week is the bare minimum to make sure dependencies stay up-to-date. Alpine Linux is a better choice than most other \u0026ldquo;stable\u0026rdquo; distributions because it usually has more recent packages.\nStable distributions often rely on backporting security fixes from CVEs, which is known to be a flawed approach to security since CVEs aren\u0026rsquo;t always assigned or even taken care of. Alpine has more recent packages, and it has versioning, so it\u0026rsquo;s once again a particulary good choice as long as musl doesn\u0026rsquo;t cause issues.\nIs it really a security nightmare? When people say Docker is a security nightmare because of that, that\u0026rsquo;s a fair point. On a traditional system, you could upgrade your whole system with a single command or two. With Docker, you\u0026rsquo;ll have to recreate several containers\u0026hellip; if the images were kept up-to-date in the first place. Recreating itself is not a big deal actually: hot upgrades of binaries and libraries often require the services that use them to restart, otherwise they could still use an old (and vulnerable) version of them in memory. But yeah, the fact is most people are running outdated containers, and more often than not, they don\u0026rsquo;t have the choice if they rely on third-party images.\nTrivy is an excellent tool to scan images for a subset of known vulnerabilities an image might have. You should play with it and see for yourself how outdated many publicly available images are.\nSupply-chain attacks As with any code downloaded from a software vendor, OCI images are not exempt from supply-chain attacks. The good practice is quite simple: rely on official images, and ideally build and maintain your own images. One should definitely not automatically trust random third-party images they can find on Docker Hub. Half of these images, if not more, contain vulnerabilities, and I bet a good portion of them contains malwares such as miners or worse.\nAs an image maintainer, you can sign your images to improve the authenticity assurance. Most official images make use of Docker Content Trust, which works with a OCI registry attached to a Notary server. With the Docker toolset, setting the environment variable DOCKER_CONTENT_TRUST=1 enforces signature verification (a signature is only good if it\u0026rsquo;s checked in the first place). The SigStore initiative is developing cosign, an alternative that doesn\u0026rsquo;t require a Notary server because it works with features already provided by the registry such as tags. Kubernetes users may be interested in Connaisseur to ensure all signatures have been validated.\nLeave my root alone! Attack surface Traditionally, Docker runs as a daemon owned by root. That also means that root in the container is actually the root on the host and may be a few commands away from compromising the host. More generally, the attacker has to exploit the available attack surface to escape the container. There is a huge attack surface, actually: the Linux kernel. Someone wise once said:\nThe kernel can effectively be thought of as the largest, most vulnerable setuid root binary on the system.\nThat applies particulary to traditional containers which weren\u0026rsquo;t designed to provide a robust level of isolation. A recent example was CVE-2022-0492: the attacker could abuse root in the container to exploit cgroups v1, and compromise the host. Of course defense-in-depth measures would have prevented that, and we\u0026rsquo;ll mention them. But fundamentally, container escapes are possible by design.\nBreaking out via the OCI runtime runc is also possible, although CVE-2019-5736 was a particularly nasty bug. The attacker had to gain access to root in the container first in order to access /proc/[runc-pid]/exe, which indicates them where to overwrite the runc binary.\nGood practices have been therefore established:\nAvoid using root in the container, plain and simple. Keep the host kernel, Docker and the OCI runtime updated. Consider the usage of user namespaces. By the way, it goes without saying that any user who has access to the Docker daemon should be considered as privileged as root. Mounting the Docker socket (/var/run/docker.sock) in a container makes it highly privileged, and so it should be avoided. The socket should only be owned by root, and if that doesn\u0026rsquo;t work with your environment, use Docker rootless or Podman.\nAvoiding root root can be avoided in different ways in the final container:\nImage creation time: setting the USER instruction in the Dockerfile. Container creation time: via the tools available (user: in the Compose file). Container runtime: degrading privileges with entrypoints scripts (gosu UID:GID). Well-made images with security in mind will have a USER instruction. In my experience, most people will run images blindly, so it\u0026rsquo;s good harm reduction. Setting the user manually works in some images that aren\u0026rsquo;t designed without root in mind, and it\u0026rsquo;s also great to mitigate some scenarii where the image is controlled by an attacker. You also won\u0026rsquo;t have surprises when mounting volumes, so I highly recommend setting the user explicitly and make sure volume permissions are correct once.\nSome images allow users to define their own user with UID/GID environment variables, with an entrypoint script that runs as root and takes care of the volume permissions before dropping privileges. While technically fine, it is still attack surface, and it requires the SETUID/SETGID capabilities to be available in the container.\nUser namespaces: sandbox or paradox? As mentioned just above, user namespaces are a solution to ensure root in the container is not root on the host. Docker supports user namespaces, for instance you could set the default mapping in /etc/docker/daemon.json:\n\u0026#34;userns-remap\u0026#34;: \u0026#34;default\u0026#34; whoami \u0026amp;\u0026amp; sleep 60 in the container will return root, but ps -fC sleep on the host will show us the PID of another user. That is nice, but it has limitations and therefore shouldn\u0026rsquo;t be considered as a real sandbox. In fact, the paradox is that user namespaces are attack surface (and vulnerabilities are still being found years later), and it\u0026rsquo;s common wisdom to restrict them to privileged users (kernel.unprivileged_userns_clone=0). That is fine for Docker with its traditional root daemon, but Podman expects you to let unprivileged users interact with user namespaces (so essentially privileged code).\nEnabling userns-remap in Docker shouldn\u0026rsquo;t be a substitute for running unprivileged application containers (where applicable). User namespaces are mostly useful if you intend to run full-fledged OS containers which need root in order to function, but that is out of the scope of the container technologies mentioned in this article; for them, I\u0026rsquo;d argue exposing such a vulnerable attack surface from the host kernel for dubious sandboxing benefits isn\u0026rsquo;t an interesting trade-off to make.\nThe no_new_privs bit After ensuring root isn\u0026rsquo;t used in your containers, you should look into setting the no_new_privs bit. This Linux feature restricts syscalls such as execve() from granting privileges, which is what you want to restrict in-container privilege escalation. This flag can be set for a given container in a Compose file:\nsecurity_opt: - no-new-privileges: true Gaining privileges in the container will be much harder that way.\nCapabilities Furthermore, we should mention capabilities: root powers are divided into distinct units by the Linux kernel, called capabilities. Each granted capability also grants privilege and therefore access to a significant amount of attack surface. Security researcher Brad Spengler enumerates 19 important capabilities. Docker restricts certain capabilities by default, but some of the most important ones are still available to a container by default.\nYou should consider the following rule of thumb:\nDrop all capabilities by default. Allow only the ones you really need to. If you already run your containers unprivileged without root, your container will very likely work fine with all capabilities dropped. That can be done in a Compose file:\ncap_drop: - ALL #cap_add: # - CHOWN # - DAC_READ_SEARCH # - SETUID # - SETGID Never use the --privileged option unless you really need to: a privileged container is given access to almost all capabilities, kernel features and devices.\nOther security features MACs and seccomp are robust tools that may vastly improve container security.\nMandatory Access Control MAC stand for Mandatory Access Control: traditionnally a Linux Security Module that will enforce a policy to restrict the userspace. Examples are AppArmor and SELinux: the former being more easy-to-use, the later being more fine-grained. Both are strong tools that can help\u0026hellip; Yet, their sole presence does not mean they\u0026rsquo;re really effective. A robust policy starts from a deny all policy, and only allows the necessary resources to be accessed.\nseccomp seccomp (short for secure computing mode) on the other hand is a much simpler and complementary tool, and there is no reason not to use it. What it does is restricting a process to a set of system calls, thus drastically reducing the attack surface available.\nDocker provides default profiles for AppArmor and seccomp, and they\u0026rsquo;re enabled by default for newly created containers unless the unconfined option is explicitly passed. Note: Kubernetes doesn\u0026rsquo;t enable the default seccomp profile by default, so you should probably try it.\nThese profiles are a great start, but you should do much more if you take security seriously, because they were made to not break compatibility with a large range of images. The default seccomp profile only disables around 44 syscalls, which are mostly not very common and/or obsoleted. Of course, the best profile you can get is supposed to be written for a given program. It also doesn\u0026rsquo;t make sense to insist on the permissiveness of the default profiles, and a lof of work has gone into hardening containers.\ncgroups Use cgroups to restrict access to hardware and system resources. You likely don\u0026rsquo;t want a guest container to monopolize the host resources. You also don\u0026rsquo;t want to be vulnerable to stupid fork bomb attacks. In a Compose file, consider setting these limits:\nmem_limit: 4g cpus: 4 pids_limit: 256 More runtime options can be found in the official documentation. All of them should have a Compose spec equivalent.\nThe --cgroup-parent option should be avoided as it uses the host cgroup and not the one configured from Docker (or else), which is the default.\nRead-only filesystem It is good practice to treat the image as some refer to as the \u0026ldquo;golden image\u0026rdquo;.\nIn other words, you\u0026rsquo;ll run containers in read-only mode, with an immutable filesystem inherited from the image. Only the mounted volumes will be read/write accessible, and those should ideally be mounted with the noexec, nosuid and nodev options for extra security. If read/write access isn\u0026rsquo;t needed, mount these volumes as read-only too.\nHowever, the image may not be perfect and still require read/write access to some parts of the filesystem, likely directories such as /tmp, /run or /var. You can make a tmpfs for those (a temporary filesystem in the container attributed memory), because they\u0026rsquo;re not persistent data anyway.\nIn a Compose file, that would look like the following settings:\nread_only: true tmpfs: - /tmp:size=10M,mode=0770,uid=1000,gid=1000,noexec,nosuid,nodev That is quite verbose indeed, but that\u0026rsquo;s to show you the different options for a tmpfs mount. You want to restrict them in size and permissions ideally.\nNetwork isolation By default, all Docker containers will use the default network bridge. They will see and be able to communicate with each other. Each container should have its own user-defined bridge network, and each connection between containers should have an internal network. If you intend to run a reverse proxy in front of several containers, you should make a dedicated network for each container you want to expose to the reverse proxy.\nThe --network host option also shouldn\u0026rsquo;t be used for obvious reasons since the container would share the same network as the host, providing no isolation at all.\nAlternative runtimes (gVisor) runc is the reference OCI runtime, but that means other runtimes can exist as well as long as they\u0026rsquo;re compliant with the OCI standard. These runtimes can be interchanged quite seamlessly. There\u0026rsquo;s a few alternatives, such as crun or youki, respectively implemented in C and Rust (runc is a Go implementation). However, there is one particular runtime that does a lot more for security: runsc, provided by the gVisor project by the folks at Google.\nContainers are not a sandbox, and while we can improve their security, they will fundamentally share a common attack surface with the host. Virtual machines are a solution to that problem, but you might prefer container semantics and ecosystem. gVisor can be perceived as an attempt to get the \u0026ldquo;best of both worlds\u0026rdquo;: containers that are easy to manage while providing a native isolation boundary. gVisor did just that by implementing two things:\nSentry: an application kernel in Go, a language known to be memory-safe. It implements the Linux logic in userspace such as various system calls. Gofer: a host process which communicates with Sentry and the host filesystem, since Sentry is restricted in that aspect. A platform like ptrace or KVM is used to intercept system calls and redirect them from the application to Sentry, which is running in the userspace. This has some costs: there is a higher per-syscall overhead, and compatibility is reduced since not all syscalls are implemented. On top of that, gVisor employs security mechanisms we\u0026rsquo;ve glanced over above, such as a very restrictive seccomp profile between Sentry and the host kernel, the no_new_privs bit, and isolated namespaces from the host.\nThe security model of gVisor is comparable to what you would expect from a virtual machine. It is also very easy to install and use. The path to runsc along with its different configuration flags (runsc flags) should be added to /etc/docker/daemon.json:\n\u0026#34;runtimes\u0026#34;: { \u0026#34;runsc-ptrace\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/runsc\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--platform=ptrace\u0026#34; ] }, \u0026#34;runsc-kvm\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/runsc\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--platform=kvm\u0026#34; ] } } runsc needs to start with root to set up some mitigations, including the use of its own network stack separated from the host. The sandbox itself drops privileges to nobody as soon as possible. You can still use runsc rootless if you want (which should be needed for Podman):\n./runsc --rootless do uname -a *** Warning: sandbox network isn\u0026#39;t supported with --rootless, switching to host *** Linux 4.4.0 #1 SMP Sun Jan 10 15:06:54 PST 2016 x86_64 GNU/Linux Linux 4.4.0 is shown because that is the version of the Linux API that Sentry tries to mimic. As you\u0026rsquo;ve probably guessed, you\u0026rsquo;re not really using Linux 4.4.0, but the application kernel that behaves like it. By the way, gVisor is of course compatible with cgroups.\nConclusion: what\u0026rsquo;s a container after all? Like I wrote above, a container is mostly defined by its semantics and ecosystem. Containers shouldn\u0026rsquo;t be solely defined by the OCI reference runtime implementation, as we\u0026rsquo;ve seen with gVisor that provides an entirely different security model.\nStill not convinced? What if I told you a container can leverage the same technologies as a virtual machine? That is exactly what Kata Containers does by using a VMM like QEMU-lite to provide containers that are in fact lightweight virtual machines, with their traditional resources and security model, compatibility with container semantics and toolset, and an optimized overhead. While not in the OCI ecosystem, Amazon achieves quite the same with Firecracker.\nIf you\u0026rsquo;re running untrusted workloads, I highly suggest you consider gVisor instead of a traditional container runtime. Your definition of \u0026ldquo;untrusted\u0026rdquo; may vary: for me, almost everything should be considered untrusted. That is how modern security works, and how mobile operating systems work. It\u0026rsquo;s quite simple, security should be simple, and gVisor simply offers native security.\nContainers are a popular, yet strange world. They revolutionized the way we make and deploy software, but one should not loose the sight of what they really are and aren\u0026rsquo;t. This hardening guide is non-exhaustive, but I hope it can make you aware of some aspects you\u0026rsquo;ve never thought of.\n","permalink":"https://wonderfall.dev/docker-hardening/","summary":"Containers aren\u0026rsquo;t that new fancy thing anymore, but they were a big deal. And they still are. They are a concrete solution to the following problem:\n- Hey, your software doesn\u0026rsquo;t work\u0026hellip;\n- Sorry, it works on my computer! Can\u0026rsquo;t help you.\nWhether we like them or not, containers are here to stay. Their expressiveness and semantics allow for an abstraction of the OS dependencies that a software has, the latter being often dynamically linked against certain libraries.","title":"Docker and OCI: a humble hardening guide"},{"content":"F-Droid is a popular alternative app repository for Android, especially known for its main repository dedicated to free and open-source software. F-Droid is often recommended among security and privacy enthusiasts, but how does it stack up against Play Store in practice? This write-up will attempt to emphasize major security issues with F-Droid that you should consider.\nBefore we start, a few things to keep in mind:\nThe main goal of this write-up was to inform users so they can make responsible choices, not to trash someone else\u0026rsquo;s work. I have respect for any work done in the name of good intentions. Likewise, please don\u0026rsquo;t misinterpret the intentions of this article. You have your own reasons for using open-source or free/libre/whatever software which won\u0026rsquo;t be discussed here. A development model shouldn\u0026rsquo;t be an excuse for bad practices and shouldn\u0026rsquo;t lure you into believing that it can provide strong guarantees it cannot. A lot of information in this article is sourced from official and trusted sources, but you\u0026rsquo;re welcome to do your own research. These analyses do not account for threat models and personal preferences. As the author of this article, I\u0026rsquo;m only interested in facts and not ideologies. This is not an in-depth security review, nor is it exhaustive.\n1. The trusted party problem To understand why this is a problem, you\u0026rsquo;ll have to understand a bit about F-Droid\u0026rsquo;s architecture, the things it does very differently from other app repositories, and the Android platform security model (some of the issues listed in this article are somewhat out of the scope of the OS security model, but the majority is).\nUnlike other repositories, F-Droid signs all the apps in the main repository with its own signing keys (unique per app) at the exception of the very few reproducible builds. A signature is a mathematical scheme that guarantees the authenticity of the applications you download. Upon the installation of an app, Android pins the signature across the entire OS (including user profiles): that\u0026rsquo;s what we call a trust-on-first-use model since all subsequent updates of the app must have the corresponding signature to be installed.\nNormally, the developer is supposed to sign their own app prior to its upload on a distribution channel, whether that is a website or a traditional repository (or both). You don\u0026rsquo;t have to trust the source (usually recommended by the developer) except for the first installation: future updates will have their authenticity cryptographically guaranteed. The issue with F-Droid is that all apps are signed by the same party (F-Droid) which is also not the developer. You\u0026rsquo;re now adding another party you\u0026rsquo;ll have to trust since you still have to trust the developer anyway, which isn\u0026rsquo;t ideal: the fewer parties, the better.\nOn the other hand, Play Store now manages the app signing keys too, as Play App Signing is required for app bundles which are required for new apps since August 2021. These signing keys can be uploaded or automatically generated, and are securely stored by Google Cloud Key Management Service. It should be noted that the developer still has to sign the app with an upload key so that Google can verify its authenticity before signing it with the app signing key. For apps created before August 2021 that may have not opted in Play App Signing yet, the developer still manages the private key and is responsible for its security, as a compromised private key can allow a third party to sign and distribute malicious code.\nF-Droid requires that the source code of the app is exempt from any proprietary library or ad service, according to their inclusion policy. Usually, that means that some developers will have to maintain a slightly different version of their codebase that should comply with F-Droid\u0026rsquo;s requirements. Besides, their \u0026ldquo;quality control\u0026rdquo; offers close to no guarantees as having access to the source code doesn\u0026rsquo;t mean it can be easily proofread. Saying Play Store is filled with malicious apps is beyond the point: the false sense of security is a real issue. Users should not think of the F-Droid main repository as free of malicious apps, yet unfortunately many are inclined to believe this.\nBut\u0026hellip; can\u0026rsquo;t I just trust F-Droid and be done with it?\nYou don\u0026rsquo;t have to take my word for it: they openly admit themselves it\u0026rsquo;s a very basic process relying on badness enumeration (this doesn\u0026rsquo;t work by the way) which consists in a few scripts scanning the code for proprietary blobs and known trackers. You are therefore not exempted from trusting upstream developers and it goes for any repository.\nA tempting idea would be to compare F-Droid to the desktop Linux model where users trust their distribution maintainers out-of-the-box (this can be sane if you\u0026rsquo;re already trusting the OS anyway), but the desktop platform is intrinsically chaotic and heterogeneous for better and for worse. It really shouldn\u0026rsquo;t be compared to the Android platform in any way.\nWhile we\u0026rsquo;ve seen that F-Droid controls the signing servers (much like Play App Signing), F-Droid also fully controls the build servers that run the disposable VMs used for building apps. And as of July 2022, their guest VM image officially runs a version of Debian which reached EOL. Undoubtedly, this raises questions about their whole infrastructure security.\nHow can you be sure that the app repository can be held to account for the code it delivers?\nF-Droid\u0026rsquo;s answer, interesting yet largely unused, is build reproducibility. While deterministic builds are a neat idea in theory, it requires the developer to make their toolchain match with what F-Droid provides. It\u0026rsquo;s additional work on both ends sometimes resulting in apps severely lagging behind in updates, so reproducible builds are not as common as we would have wanted. It should be noted that reproducible builds in the main repository can be exclusively developer-signed.\nGoogle\u0026rsquo;s approach is code transparency for app bundles, which is a simple idea addressing some of the concerns with Play App Signing. A JSON Web Token (JWT) signed by a key private to the developer is included in the app bundle before its upload to Play Store. This token contains a list of DEX files and native .so libraries and their hashes, allowing end-users to verify that the running code was built and signed by the app developer. Code transparency has known limitations, however: not all resources can be verified, and this verification can only be done manually since it\u0026rsquo;s not part of the Android platform itself (so requiring a code transparency file cannot be enforced by the OS right now). Despite its incompleteness, code transparency is still helpful, easy to implement, and thus something we should see more often as time goes by.\nWhat about other app repositories such as Amazon?\nTo my current knowledge, the Amazon Appstore has always been wrapping APKs with their own code (including their own trackers), and this means they were effectively resigning submitted APKs.\nIf you understood correctly the information above, Google can\u0026rsquo;t do this for apps that haven\u0026rsquo;t opted in Play App Signing. As for apps concerned by Play App Signing, while Google could technically introduce their own code like Amazon, they wouldn\u0026rsquo;t do that without telling about it since this will be easily noticeable by the developer and more globally researchers. They have other means on the Android app development platform to do so. Believing they won\u0026rsquo;t do that based on this principle is not a strong guarantee, however: hence the above paragraph about code transparency for app bundles.\nHuawei AppGallery seems to have a similar approach to Google, where submitted apps could be developer-signed, but newer apps will be resigned by Huawei.\n2. Slow and irregular updates Since you\u0026rsquo;re adding one more party to the mix, that party is now responsible for delivering proper builds of the app: it\u0026rsquo;s a common thing among traditional Linux distributions and their packaging system. They have to catch up with upstream on a regular basis, but very few do it well (Arch Linux comes to my mind). Others, like Debian, prefer making extensive downstream changes and delivering security fixes for a subset of vulnerabilities assigned to a CVE (yeah, it\u0026rsquo;s as bad as it sounds, but that\u0026rsquo;s another topic).\nNot only does F-Droid require specific changes for the app to comply with its inclusion policy, which often leads to more maintenance work, it also has a rather strange way of triggering new builds. Part of its build process seems to be automated, which is the least you could expect. Now here\u0026rsquo;s the thing: app signing keys are on an air-gapped server (meaning it\u0026rsquo;s disconnected from any network, at least that\u0026rsquo;s what they claim: see their recommendations for reference), which forces an irregular update cycle where a human has to manually trigger the signing process. It is far from an ideal situation, and you may argue it\u0026rsquo;s the least to be expected since by entrusting all the signing keys to one party, you could also introduce a single point of failure. Should their system be compromised (whether from the inside or the outside), this could lead to serious security issues affecting plenty of users.\nThis is one of the main reasons why Signal refused to support the inclusion of a third-party build in the F-Droid official repository. While this GitHub issue is quite old, many points still hold true today.\nConsidering all this, and the fact that their build process is often broken using outdated tools, you have to expect far slower updates compared to a traditional distribution system. Slow updates mean that you will be exposed to security vulnerabilities more often than you should\u0026rsquo;ve been. It would be unwise to have a full browser updated through the F-Droid official repository, for instance. F-Droid third-party repositories somewhat mitigate the issue of slow updates since they can be managed directly by the developer. It isn\u0026rsquo;t ideal either as you will see below.\n3. Low target API level (SDK) for client \u0026amp; apps SDK stands for Software Development Kit and is the collection of software to build apps for a given platform. On Android, a higher SDK level means you\u0026rsquo;ll be able to make use of modern API levels of which each iteration brings security and privacy improvements. For instance, API level 31 makes use of all these improvements on Android 12.\nAs you may already know, Android has a strong sandboxing model where each application is sandboxed. You could say that an app compiled with the highest API level benefits from all the latest improvements brought to the app sandbox; as opposed to outdated apps compiled with older API levels, which have a weaker sandbox.\n# b/35917228 - /proc/misc access # This will go away in a future Android release allow untrusted_app_25 proc_misc:file r_file_perms; # Access to /proc/tty/drivers, to allow apps to determine if they # are running in an emulated environment. # b/33214085 b/33814662 b/33791054 b/33211769 # https://github.com/strazzere/anti-emulator/blob/master/AntiEmulator/src/diff/strazzere/anti/emulator/FindEmulator.java # This will go away in a future Android release allow untrusted_app_25 proc_tty_drivers:file r_file_perms; This is a mere sample of the SELinux exceptions that have to be made on older API levels so that you can understand why it matters.\nIt turns out the official F-Droid client doesn\u0026rsquo;t care much about this since it lags behind quite a bit, targeting the API level 25 (Android 7.1) of which some SELinux exceptions were shown above. As a workaround, some users recommended third-party clients such as Foxy Droid or Aurora Droid. While these clients might be technically better, they\u0026rsquo;re poorly maintained for some, and they also introduce yet another party to the mix. Droid-ify (recently rebreanded to Neo-Store) seems to be a better option than the official client in most aspects.\nFurthermore, F-Droid doesn\u0026rsquo;t enforce a minimum target SDK for the official repository. Play Store does that quite aggressively for new apps and app updates:\nSince August 2021, Play Store requires new apps to target at least API level 30. Since November 2021, existing apps must at least target API level 30 for updates to be submitted. While it may seem bothersome, it\u0026rsquo;s a necessity to keep the app ecosystem modern and healthy. Here, F-Droid sends the wrong message to developers (and even users) because they should care about it, and this is why many of us think it may be even harmful to the FOSS ecosystem. Backward compatibility is often the enemy of security, and while there\u0026rsquo;s a middle-ground for convenience and obsolescence, it shouldn\u0026rsquo;t be exaggerated. As a result of this philosophy, the main repository of F-Droid is filled with obsolete apps from another era, just for these apps to be able to run on the more than ten years old Android 4.0 Ice Cream Sandwich. Let\u0026rsquo;s not make the same mistake as the desktop platforms: instead, complain to your vendors for selling devices with no decent OS/firmware support.\nThere is little practical reason for developers not to increase the target SDK version (targetSdkVersion) along with each Android release. This attribute matches the version of the platform an app is targeting, and allows access to modern improvements, rules and features on a modern OS. The app can still ensure backwards compatibility in such a way that it can run on older platforms: the minSdkVersion attribute informs the system about the minimum API level required for the application to run. Setting it too low isn\u0026rsquo;t practical though, because this requires having a lot of fallback code (most of it is handled by common libraries) and separate code paths.\nAt the time of writing:\nAndroid 9 is the oldest Android version that is getting security updates. ~80% of the Android devices used in the world are at least running 8.0 Oreo. Overall statistics do not reflect real-world usage of a given app (people using old devices are not necessarily using your app). If anything, it should be viewed as an underestimation.\n4. General lack of good practices The F-Droid client allows multiple repositories to coexist within the same app. Many of the issues highlighted above were focused on the main official repository which most of the F-Droid users will use anyway. However, having other repositories in a single app also violates the security model of Android which was not designed for this at all. The OS expects you to trust an app repository as a single source of apps, yet F-Droid isn\u0026rsquo;t that by design as it mixes several repositories in one single app. This is important because the OS management APIs and features (such as UserManager which can be used to prevent a user from installing third-party apps) are not meant for this and see F-Droid as a single source, so you\u0026rsquo;re trusting the app client to not mess up far more than you should, especially when the privileged extension comes into the picture.\nThere is indeed a serious security issue with the OS first-party source feature being misused, as the privileged extension makes use of the INSTALL_PACKAGES API in an insecure manner (i.e. not implementing it with the appropriate security checks). The privileged extension accepts any request from F-Droid, which again suffers from various bugs and security issues and allows user-defined repositories by design. A lot can go wrong, and bypassing security checks for powerful APIs should definitely not be taken lightly.\nOn that note, it is also worth noting the repository metadata format isn\u0026rsquo;t properly signed by lacking whole-file signing and key rotation. Their index v1 format uses JAR signing with jarsigner, which has serious security flaws. It seems that work is in progress on a v2 format with support for apksigner, although the final implementation remains to be seen. This just seems to be an over-engineered and flawed approach since better suited tools such as signify could be used to sign the metadata JSON.\nAs a matter of fact, the new unattended update API added in API level 31 (Android 12) that allows seamless app updates for app repositories without privileged access to the system (such an approach is not compatible with the security model) won\u0026rsquo;t work with F-Droid \u0026ldquo;as is\u0026rdquo;. It should be mentioned that the aforementioned third-party client Neo-Store supports this API, although the underlying issues about the F-Droid infrastructure largely remain. Indeed, this secure API allowing for unprivileged unattended updates not only requires for the app repository client to target API level 31, but the apps to be updated also have to at least target API level 29.\nTheir client also lacks TLS certificate pinning, unlike Play Store which improves security for all connections to Google (they generally use a limited set of root CAs including their own). Certificate pinning is a way for apps to increase the security of their connection to services by providing a set of public key hashes of known-good certificates for these services instead of trusting pre-installed CAs. This can avoid some cases where an interception (man-in-the-middle attack) could be possible and lead to various security issues considering you\u0026rsquo;re trusting the app to deliver you other apps.\nIt is an important security feature that is also straightforward to implement using the declarative network security configuration available since Android 7.0 (API level 24). See how GrapheneOS pins both root and CA certificates in their app repository client:\n\u0026lt;!-- res/xml/network_security_config.xml --\u0026gt; \u0026lt;network-security-config\u0026gt; \u0026lt;base-config cleartextTrafficPermitted=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;domain-config\u0026gt; \u0026lt;domain includeSubdomains=\u0026#34;true\u0026#34;\u0026gt;apps.grapheneos.org\u0026lt;/domain\u0026gt; \u0026lt;pin-set\u0026gt; \u0026lt;!-- ISRG Root X1 --\u0026gt; \u0026lt;pin digest=\u0026#34;SHA-256\u0026#34;\u0026gt;C5+lpZ7tcVwmwQIMcRtPbsQtWLABXhQzejna0wHFr8M=\u0026lt;/pin\u0026gt; \u0026lt;!-- ISRG Root X2 --\u0026gt; \u0026lt;pin digest=\u0026#34;SHA-256\u0026#34;\u0026gt;diGVwiVYbubAI3RW4hB9xU8e/CH2GnkuvVFZE8zmgzI=\u0026lt;/pin\u0026gt; \u0026lt;!-- Let\u0026#39;s Encrypt R3 --\u0026gt; \u0026lt;pin digest=\u0026#34;SHA-256\u0026#34;\u0026gt;jQJTbIh0grw0/1TkHSumWb+Fs0Ggogr621gT3PvPKG0=\u0026lt;/pin\u0026gt; \u0026lt;!-- Let\u0026#39;s Encrypt E1 --\u0026gt; \u0026lt;pin digest=\u0026#34;SHA-256\u0026#34;\u0026gt;J2/oqMTsdhFWW/n85tys6b4yDBtb6idZayIEBx7QTxA=\u0026lt;/pin\u0026gt; ... \u0026lt;/pin-set\u0026gt; \u0026lt;/domain-config\u0026gt; \u0026lt;/network-security-config\u0026gt; To be fair, they\u0026rsquo;ve thought several times about adding certificate pinning to their client at least for the default repositories. Relics of preliminary work can even be found in their current codebase, but it\u0026rsquo;s unfortunate that they haven\u0026rsquo;t been able to find any working implementation so far. Given the overly complex nature of F-Droid, that\u0026rsquo;s largely understandable.\nF-Droid also has a problem regarding the adoption of new signature schemes as they held out on the v1 signature scheme (which was horrible and deprecated since 2017) until they were forced by Android 11 requirements to support the newer v2/v3 schemes (v2 was introduced in Android 7.0). Quite frankly, this is straight-up bad, and signing APKs with GPG is no better considering how bad PGP and its reference implementation GPG are (even Debian is trying to move away from it). Ideally, F-Droid should fully move on to newer signature schemes, and should completely phase out the legacy signature schemes which are still being used for some apps and metadata.\n5. Confusing UX It is worth mentioning that their website has (for some reason) always been hosting an outdated APK of F-Droid, and this is still the case today, leading to many users wondering why they can\u0026rsquo;t install F-Droid on their secondary user profile (due to the downgrade prevention enforced by Android). \u0026ldquo;Stability\u0026rdquo; seems to be the main reason mentioned on their part, which doesn\u0026rsquo;t make sense: either your version isn\u0026rsquo;t ready to be published in a stable channel, or it is and new users should be able to access it easily.\nF-Droid should enforce the approach of prefixing the package name of their alternate builds with org.f-droid for instance (or add a .fdroid suffix as some already have). Building and signing while reusing the package name (application ID) is bad practice as it causes signature verification errors when some users try to update/install these apps from other sources, even directly from the developer. That is again due to the security model of Android which enforces a signature check when installing app updates (or installing them again in a secondary user profile). Note that this is going to be an issue with Play App Signing as well, and developers are encouraged to follow this approach should they intend to distribute their apps through different distribution channels.\nThis results in a confusing user experience where it\u0026rsquo;s hard to keep track of who signs each app, and from which repository the app should be downloaded or updated.\n6. Misleading permissions approach F-Droid shows a list of the low-level permissions for each app: these low-level permissions are usually grouped in the standard high-level permissions (Location, Microphone, Camera, etc.) and special toggles (nearby Wi-Fi networks, Bluetooth devices, etc.) that are explicitly based on a type of sensitive data. While showing a list of low-level permissions could be useful information for a developer, it\u0026rsquo;s often a misleading and inaccurate approach for the end-user. Since Android 6, apps have to request the standard permissions at runtime and do not get them simply by being installed, so showing all the \u0026ldquo;under the hood\u0026rdquo; permissions without proper context is not useful and makes the permission model unnecessarily confusing.\nF-Droid claims that these low-level permissions are relevant because they support Android 5.1+, meaning they support very outdated versions of Android where apps could have install-time permissions. Anyway, if a technical user wants to see all the manifest permissions for some reason, then they can access the app manifest pretty easily (in fact, exposing the raw manifest would be less misleading). But this is already beyond the scope of this article because anyone who cares about privacy and security wouldn\u0026rsquo;t run a 8 years old version of Android that has not received security updates for years.\nTo clear up confusion: even apps targeting an API level below 23 (Android 5.1 or older) do not have permissions granted at install time on modern Android, which instead displays a legacy permission grant dialog. Whether or not permissions are granted at install time does not just depend on the app\u0026rsquo;s targetSdkVersion. And even if this were the case, the OS package installer on modern Android would\u0026rsquo;ve been designed to show the requested permissions for those legacy apps.\nFor example, the low-level permission RECEIVE_BOOT_COMPLETED is referred to in F-Droid as the run at startup description, when in fact this permission is not needed to start at boot and just refers to a specific time broadcasted by the system once it finishes booting, and is not about background usage (though power usage may be a valid concern). To be fair, these short summaries used to be provided by the Android documentation years ago, but the permission model has drastically evolved since then and most of them aren\u0026rsquo;t accurate anymore.\nAllows the app to have itself started as soon as the system has finished booting. This can make it take longer to start the phone and allow the app to slow down the overall phone by always running.\nIn modern Android, the background restriction toggle is what really provides the ability for apps to run in the background. Some low-level permissions don\u0026rsquo;t even have a security/privacy impact and shouldn\u0026rsquo;t be misinterpreted as having one. Anyhow, you can be sure that each dangerous low-level permission has a high-level representation that is disabled by default and needs to be granted dynamically to the app (by a toggle or explicit user consent in general).\nAnother example to illustrate the shortcomings of this approach would be the QUERY_ALL_PACKAGES low-level permission, which is referred to as the query all packages permission that \u0026ldquo;allows an app to see all installed packages\u0026rdquo;. While this is somewhat correct, this can also be misleading: apps do not need QUERY_ALL_PACKAGES to list other apps within the same user profile. Even without this permission, some apps are visible automatically (visibility is restricted by default since Android 11). If an app needs more visibility, it will declare a \u0026lt;queries\u0026gt; element in its manifest file: in other words, QUERY_ALL_PACKAGES is only one way to achieve visibility. Again, this goes to show low-level manifest permissions are not intended to be interpreted as high-level permissions the user should fully comprehend.\nPlay Store for instance conveys the permissions in a way less misleading way: the main low-level permissions are first grouped in their high-level user-facing toggles, and the rest is shown under \u0026ldquo;Other\u0026rdquo;. This permission list can only be accessed by taping \u0026ldquo;About this app\u0026rdquo; then \u0026ldquo;App permissions - See more\u0026rdquo; at the bottom of the page. Play Store will tell the app may request access to the following permissions: this kind of wording is more important than it seems. Update: since July 2022, Play Store doesn\u0026rsquo;t offer a way to display low-level permissions anymore.\nMoreover, Play Store restricts the use of highly invasive permissions such as MANAGE_EXTERNAL_STORAGE which allows apps to opt out of scoped storage if they can\u0026rsquo;t work with more privacy friendly approaches (like a file explorer). Apps that can\u0026rsquo;t justify their use of this permission (which again has to be granted dynamically) may be removed from Play Store. This is where an app repository can actually be useful in their review process to protect end-users from installing poorly made apps that might compromise their privacy. Not that it matters much if these apps target very old API levels that are inclined to require invasive permissions in the first place\u0026hellip;\nConclusion: what should you do? So far, you have been presented with referenced facts that are easily verifiable. In the next part, I\u0026rsquo;ll allow myself to express my own thoughts and opinions. You\u0026rsquo;re free to disagree with them, but don\u0026rsquo;t let that overshadow the rest.\nWhile some improvements could easily be made, I don\u0026rsquo;t think F-Droid is in an ideal situation to solve all of these issues because some of them are inherent flaws in their architecture. I\u0026rsquo;d also argue that their core philosophy is not aligned with some security principles expressed in this article. In any case, I can only wish for them to improve since they\u0026rsquo;re one of the most popular alternatives to commercial app repositories, and are therefore trusted by a large userbase.\nF-Droid is often seen as the only way to get and support open-source apps: that is not the case. Sure, F-Droid could help you in finding FOSS apps that you wouldn\u0026rsquo;t otherwise have known existed. Many developers also publish their FOSS apps on the Play Store or their website directly. Most of the time, releases are available on GitHub, which is great since each GitHub releases page has an Atom feed. If downloading APKs from regular websites, you can use apksigner to validate the authenticity by comparing the certificate fingerprint against the fingerprint from another source (it wouldn\u0026rsquo;t matter otherwise).\nThis is how you may proceed to get the app certificate:\napksigner verify --print-certs --verbose myCoolApp.apk Also, as written above: the OS pins the app signature (for all profiles) upon installation, and enforces signature check for app updates. In practice, this means the source doesn\u0026rsquo;t matter as much after the initial installation.\nFor most people, I\u0026rsquo;d recommend just sticking with Play Store. Play Store isn\u0026rsquo;t quite flawless, but emphasises the adoption of modern security standards which in turn encourages better privacy practices; as strange as it may sound, Google is not always doing bad things in that regard.\nNote: this article obviously can\u0026rsquo;t address all the flaws related to Play Store itself. Again, the main topic of this article is about F-Droid and should not be seen as an exhaustive comparison between different app repositories.\nShould I really care?\nIt\u0026rsquo;s up to your threat model, and of course your personal preferences. Most likely, your phone won\u0026rsquo;t turn into a nuclear weapon if you install F-Droid on it - and this is far from the point that this article is trying to make. Still, I believe the information presented will be valuable for anyone who values a practical approach to privacy (rather than an ideological one). Such an approach is partially described below.\nBut there is more malware in Play Store! How can you say that it\u0026rsquo;s more secure?\nAs explained above, it doesn\u0026rsquo;t matter as you shouldn\u0026rsquo;t really rely on any quality control to be the sole guarantee that a software is free of malicious or exploitable code. Play Store and even the Apple App Store may have a considerable amount of malware because a full reverse-engineering of any uploaded app isn\u0026rsquo;t feasible realistically. However, they fulfill their role quite well, and that is all that is expected of them.\nWith Play App Signing being effectively enforced for new apps, isn\u0026rsquo;t Play Store as \u0026ldquo;flawed\u0026rdquo; as F-Droid?\nI\u0026rsquo;ve seen this comment repeatedly, and it would be dismissing all the other points made in this article. Also, I strongly suggest that you carefully read the sections related to Play App Signing, and preferably the official documentation on this matter. It\u0026rsquo;s not a black and white question and there are many more nuances to it.\nAren\u0026rsquo;t open-source apps more secure? Doesn\u0026rsquo;t it make F-Droid safer?\nYou can still find and get your open-source apps elsewhere. And no, open-source apps aren\u0026rsquo;t necessarily more private or secure. Instead, you should rely on the strong security and privacy guarantees provided by a modern operating system with a robust sandboxing/permission model, namely modern Android, GrapheneOS and iOS. Pay close attention to the permissions you grant, and avoid legacy apps as they could require invasive permissions to run.\nWhen it comes to trackers (this really comes up a lot), you shouldn\u0026rsquo;t believe in the flawed idea that you can enumerate all of them. The enumerating badness approach is known to be flawed in the security field, and the same applies to privacy. You shouldn\u0026rsquo;t believe that a random script can detect every single line of code that can be used for data exfiltration. Data exfiltration can be properly prevented in the first place by the permission model, which again denies access to sensitive data by default: this is a simple, yet rigorous and effective approach.\nNo app should be unnecessarily entrusted with any kind of permission. It is only if you deem it necessary that you should allow access to a type of data, and this access should be as fine-grained as possible. That\u0026rsquo;s the way the Android platform works (regular apps run in the explicit untrusted_app domain) and continues evolving. Contrary to some popular beliefs, usability and most productivity tasks can still be achieved in a secure and private way.\nIsn\u0026rsquo;t Google evil? Isn\u0026rsquo;t Play Store spyware?\nSome people tend to exaggerate the importance of Google in their threat model, at the cost of pragmatism and security/privacy good practices. Play Store isn\u0026rsquo;t spyware and can run unprivileged like it does on GrapheneOS (including with unattended updates support). On the vast majority of devices though, Google Play is a privileged app and a core part of the OS that provides low-level system modules. In that case, the trust issues involved with Play App Signing could be considered less important since Google Play is already trusted as a privileged component.\nPlay Store evidently has some privacy issues given it\u0026rsquo;s a proprietary service which requires an account (this cannot be circumvented), and Google services have a history of nagging users to enable privacy-invasive features. Again, some of these privacy issues can be mitigated by setting up the Play services compatibility layer from GrapheneOS which runs Play services and Play Store in the regular app sandbox (the untrusted_app domain). ProtonAOSP also shares that feature. This solution could very well be ported to other Android-based operating systems. If you want to go further, consider using a properly configured account with the least amount of personally indentifiable information possible (note that the phone number requirement appears to be region-dependent).\nIf you don\u0026rsquo;t have Play services installed, you can use a third-party Play Store client called Aurora Store. Aurora Store has some issues of its own, and some of them overlap in fact with F-Droid. Aurora Store somehow still requires the legacy storage permission, has yet to implement certificate pinning, has been known to sometimes retrieve wrong versions of apps, and distributed account tokens over cleartext HTTP until fairly recently; not that it matters much since tokens were designed to be shared between users, which is already concerning. I\u0026rsquo;d recommend against using the shared \u0026ldquo;anonymous\u0026rdquo; accounts feature: you should make your own throwaway account with minimal information.\nYou should also keep an eye on the great work GrapheneOS does on their future app repository. It will be a simple, secure, modern app repository for a curated list of high-quality apps, some of which will have their own builds (for instance, Signal still uses their original 1024-bits RSA key that has never been rotated since then). Inspired by this work, a GrapheneOS community member is developing a more generic app repository called Accrescent. Hopefully, we\u0026rsquo;ll see well-made alternatives like these flourish.\nThanks to the GrapheneOS community for proofreading this article. Bear in mind that these are not official recommendations from the GrapheneOS project.\nPost-publication note: it\u0026rsquo;s unfortunate that the release of this article mostly triggered a negative response from the F-Droid team which prefers to dismiss this article on several occasions rather than bringing relevant counterpoints. Some of their core members are also involved in a harassment campaign towards projects and security researchers that do not share their views. While this article remains a technical one, there are definitely ethical concerns to take into consideration.\n","permalink":"https://wonderfall.dev/fdroid-issues/","summary":"F-Droid is a popular alternative app repository for Android, especially known for its main repository dedicated to free and open-source software. F-Droid is often recommended among security and privacy enthusiasts, but how does it stack up against Play Store in practice? This write-up will attempt to emphasize major security issues with F-Droid that you should consider.\nBefore we start, a few things to keep in mind:\nThe main goal of this write-up was to inform users so they can make responsible choices, not to trash someone else\u0026rsquo;s work.","title":"A brief and informal analysis of F-Droid security"},{"content":"You may call me \u0026ldquo;Wonderfall\u0026rdquo;. I was young and it sounded cool.\n$ whoami I\u0026#39;m just a random guy passing by on the Internet who is interested in all kinds of things. And as you can tell, I\u0026#39;m a nerd. $ ls -l content/ technology security privacy rants photography pharmacology medicine science $ git config --get remote.origin.url https://github.com/Wonderfall/wonderfall.github.io ","permalink":"https://wonderfall.dev/about/","summary":"You may call me \u0026ldquo;Wonderfall\u0026rdquo;. I was young and it sounded cool.\n$ whoami I\u0026#39;m just a random guy passing by on the Internet who is interested in all kinds of things. And as you can tell, I\u0026#39;m a nerd. $ ls -l content/ technology security privacy rants photography pharmacology medicine science $ git config --get remote.origin.url https://github.com/Wonderfall/wonderfall.github.io ","title":"About"}]